{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyOUQliyYHSh"
      },
      "source": [
        "# RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_gIkBCwDAQ-",
        "outputId": "3ccf1d62-b9b2-47cb-8dc9-4f9cf3661b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/2months_data'\n",
        "all_files = os.listdir(data_folder)\n",
        "csv_files = [os.path.join(data_folder, f) for f in all_files if f.endswith('.csv')]\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    encodings = ['utf-8', 'cp949', 'euc-kr']  # List of possible encodings\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            return pd.read_csv(file_path, encoding=encoding)\n",
        "        except (UnicodeDecodeError, pd.errors.EmptyDataError, pd.errors.ParserError):\n",
        "            continue\n",
        "    raise ValueError(f\"Could not read file {file_path} with any encoding.\")\n",
        "\n",
        "valid_dataframes = []\n",
        "for file in csv_files:\n",
        "    try:\n",
        "        df = read_csv_file(file)\n",
        "        if not df.empty:\n",
        "          valid_dataframes.append(df)\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "\n",
        "if valid_dataframes:\n",
        "    combined_data = pd.concat(valid_dataframes, ignore_index=True)\n",
        "else:\n",
        "    combined_data = pd.DataFrame()\n",
        "combined_data.columns=[\"Creation date\", \"load factor\", \"Set Pressure\", \"Boiler Pressure\",\n",
        "                      \"Blower Inverter Output\", \"Blower Input\", \"Water Supply Pump\",\n",
        "                      \"Water Supply Pump Input\", \"Gas Damper\", \"Gas Damper Input\",\n",
        "                      \"Air Damper\", \"Air Damper Input\", \"Recirculation Damper\",\n",
        "                      \"Recirculation External Damper\", \"Recirculation Damper Input\",\n",
        "                      \"Recirculation External Damper Input\", \"Water Supply Level\",\n",
        "                      \"Boiler Temperature\", \"Exhaust Gas Temperature 1\",\n",
        "                      \"Exhaust Gas Temperature 2\", \"Exhaust Gas Temperature 3\",\n",
        "                      \"Exhaust Recirculation Temperature\", \"Economizer Temperature 1\",\n",
        "                      \"Economizer Temperature 2\", \"Burner Temperature\", \"Exhaust Gas NOx\",\n",
        "                      \"Exhaust Gas O2\", \"Recirculation O2\", \"Recirculation NOx\",\n",
        "                      \"Water Supply Amount (Cumulative Flow)\",\n",
        "                      \"Water Supply Amount (Instantaneous Flow)\",\n",
        "                      \"Fuel Amount (Cumulative Flow)\", \"Fuel Amount (Instantaneous Flow)\",\n",
        "                      \"Efficiency (Instantaneous)\", \"Power Consumption\", \"Vibration Sensor 1\",\n",
        "                      \"Vibration Sensor 2\", \"Operating Time\", \"Normal Operation Probability\",\n",
        "                      \"Blower Failure Probability\", \"Air Damper Failure Probability\",\n",
        "                      \"Gas Damper Failure Probability\", \"Probability Update Time\",\n",
        "                      \"Instantaneous Steam Amount\", \"Input-Output Efficiency\",\n",
        "                      \"Heat Loss Efficiency\", \"Efficiency (input/output method-steam)\"]\n",
        "combined_data = combined_data.drop(columns=[\n",
        "    \"Creation date\",\"Power Consumption\",\"Vibration Sensor 1\",\"Vibration Sensor 2\",\"Operating Time\",\n",
        "    \"Normal Operation Probability\",\"Blower Failure Probability\",\"Air Damper Failure Probability\",\n",
        "    \"Gas Damper Failure Probability\",\"Probability Update Time\",\"Instantaneous Steam Amount\",\n",
        "    \"Input-Output Efficiency\",\"Heat Loss Efficiency\",\"Efficiency (input/output method-steam)\",\n",
        "    \"Exhaust Recirculation Temperature\",\"Burner Temperature\"\n",
        "])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "numeric_columns = combined_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "combined_data[numeric_columns] = scaler.fit_transform(combined_data[numeric_columns])\n",
        "\n",
        "# Select numerical columns only\n",
        "numerical_columns = combined_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Fill missing values in numerical columns with the mean value\n",
        "combined_data[numerical_columns] = combined_data[numerical_columns].apply(lambda col: col.fillna(col.mean()))\n",
        "\n",
        "# For categorical columns, fill missing values with the most frequent value\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "combined_data[combined_data.select_dtypes(include=['object']).columns] = cat_imputer.fit_transform(combined_data.select_dtypes(include=['object']))\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "label_encoder = LabelEncoder()\n",
        "for column in combined_data.select_dtypes(include=['object']).columns:\n",
        "    combined_data[column] = label_encoder.fit_transform(combined_data[column])\n",
        "\n",
        "combined_data.to_csv('/content/drive/MyDrive/preprocessed_boiler_data.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMuvOKU7DWNu"
      },
      "outputs": [],
      "source": [
        "combined_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlK0bjwwDYIz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.stats import randint\n",
        "\n",
        "# combined_data = 전처리된 데이터프레임 (전처리 코드는 이미 실행된 상태라고 가정)\n",
        "\n",
        "# 독립 변수(X)와 종속 변수(y) 설정\n",
        "X = combined_data.drop(columns=['Efficiency (Instantaneous)'])  # 종속 변수 제외\n",
        "y = combined_data['Efficiency (Instantaneous)']\n",
        "\n",
        "# 데이터 분할: 훈련 데이터와 테스트 데이터로 분할\n",
        "# 여기에서 train_test_split을 사용하여 X_train, X_test, y_train, y_test를 생성\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 예시: 80% 훈련, 20% 테스트\n",
        "\n",
        "# Random Forest Model with RandomizedSearchCV for hyperparameter tuning\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100, 200],      # Number of trees\n",
        "    'max_depth': [None, 10, 15, 20, 25],           # Maximum depth of the tree\n",
        "    'min_samples_split': [2,5,10],   # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1,2,4],    # Minimum number of samples required to be at a leaf node\n",
        "    'max_features':['sqrt', 'log2'],  # Number of features to consider at each split\n",
        "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2]  # 리프 노드의 가중치 샘플 최소값\n",
        "}\n",
        "\n",
        "# Randomized Search CV with 5-fold cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions,\n",
        "                                   n_iter=50, cv=5, verbose=2, n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best hyperparameters found by RandomizedSearchCV:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# Model evaluation using test data\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "mae = np.mean(np.abs(y_test - y_pred))\n",
        "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
        "mse = np.mean((y_test - y_pred) ** 2)\n",
        "\n",
        "print(f\"MAPE: {mape:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE2B-1JaDZoZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.font_manager as fm\n",
        "import shutil\n",
        "\n",
        "# Seaborn 스타일 적용\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# 예시 데이터 (random_search.best_estimator_는 미리 학습된 상태여야 함)\n",
        "feature_importances = random_search.best_estimator_.feature_importances_\n",
        "features = X.columns  # 각 피처의 이름\n",
        "\n",
        "# 피처 중요도를 시리즈로 변환하고, 중요도 순서대로 정렬\n",
        "feature_importance_series = pd.Series(feature_importances, index=features).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(x=feature_importance_series, y=feature_importance_series.index)\n",
        "\n",
        "# 각 바에 중요도 값을 텍스트로 표시\n",
        "for i, (value, name) in enumerate(zip(feature_importance_series, feature_importance_series.index)):\n",
        "    ax.text(value, i, f'{value:.4f}', va='center', ha='left', fontsize=10, color='black')\n",
        "\n",
        "plt.title('Feature Importance from Random Forest Model (with values)', fontsize=14)\n",
        "plt.xlabel('Importance Score', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tokUWr_4YLQE"
      },
      "source": [
        "# GA에 사용할 RF - importance score 0.001 이상"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoDqHFPbZe2w"
      },
      "outputs": [],
      "source": [
        "# Step 1: Random Search에서 찾은 최적의 하이퍼파라미터를 적용한 새로운 Random Forest 모델 학습\n",
        "best_params = random_search.best_params_  # 이전 RandomizedSearchCV에서 찾은 최적의 하이퍼파라미터\n",
        "\n",
        "# Feature Importance가 0.001 이상인 피처들만 선택\n",
        "importance_threshold = 0.001\n",
        "important_features = feature_importance_series[feature_importance_series > importance_threshold].index\n",
        "X_filtered = X[important_features]  # 중요도가 0.001 이상인 피처들만 사용\n",
        "\n",
        "# 데이터 분할: 훈련 데이터와 테스트 데이터로 다시 분할\n",
        "X_train_filtered, X_test_filtered, y_train, y_test = train_test_split(X_filtered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 새로운 Random Forest 모델 생성 (최적 하이퍼파라미터 사용)\n",
        "rf_filtered = RandomForestRegressor(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    min_samples_split=best_params['min_samples_split'],\n",
        "    min_samples_leaf=best_params['min_samples_leaf'],\n",
        "    max_features=best_params['max_features'],\n",
        "    min_weight_fraction_leaf=best_params['min_weight_fraction_leaf'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "rf_filtered.fit(X_train_filtered, y_train)\n",
        "\n",
        "# Step 2: 모델 평가\n",
        "y_pred_filtered = rf_filtered.predict(X_test_filtered)\n",
        "\n",
        "# 성능 지표 계산\n",
        "mape_filtered = np.mean(np.abs((y_test - y_pred_filtered) / y_test)) * 100\n",
        "mae_filtered = np.mean(np.abs(y_test - y_pred_filtered))\n",
        "rmse_filtered = np.sqrt(np.mean((y_test - y_pred_filtered) ** 2))\n",
        "mse_filtered = np.mean((y_test - y_pred_filtered) ** 2)\n",
        "\n",
        "print(f\"MAPE (Filtered): {mape_filtered:.4f}\")\n",
        "print(f\"MAE (Filtered): {mae_filtered:.4f}\")\n",
        "print(f\"RMSE (Filtered): {rmse_filtered:.4f}\")\n",
        "print(f\"MSE (Filtered): {mse_filtered:.4f}\")\n",
        "\n",
        "# Step 3: Feature Importance 시각화 (필터링된 피처만)\n",
        "feature_importances_filtered = rf_filtered.feature_importances_\n",
        "\n",
        "# 피처 중요도 시리즈로 변환하고 중요도 순서대로 정렬\n",
        "feature_importance_series_filtered = pd.Series(feature_importances_filtered, index=important_features).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(x=feature_importance_series_filtered, y=feature_importance_series_filtered.index)\n",
        "\n",
        "# 각 바에 중요도 값을 텍스트로 표시\n",
        "for i, (value, name) in enumerate(zip(feature_importance_series_filtered, feature_importance_series_filtered.index)):\n",
        "    ax.text(value, i, f'{value:.4f}', va='center', ha='left', fontsize=10, color='black')\n",
        "\n",
        "plt.title('Feature Importance from Filtered Random Forest Model (with values)', fontsize=14)\n",
        "plt.xlabel('Importance Score', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzXhtdd0YFMW"
      },
      "source": [
        "# GA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2J0l-UAYTvk"
      },
      "outputs": [],
      "source": [
        "!pip install deap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TOYC3OFCL9VU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from deap import base, creator, tools, algorithms\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from joblib import parallel_backend\n",
        "\n",
        "# Step 1: 목표 손실 함수 정의 (손실 함수로 사용될 성능 지표 정의)\n",
        "def rf_loss_function(individual):\n",
        "    n_estimators = int(individual[0])\n",
        "    max_depth = int(individual[1]) if individual[1] != None else None\n",
        "    min_samples_split = int(individual[2])\n",
        "    min_samples_leaf = max(1, int(individual[3]))  # 최소 1로 설정\n",
        "    max_features = individual[4]\n",
        "    min_weight_fraction_leaf = individual[5]\n",
        "\n",
        "    # 필터링된 데이터로 모델 학습\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # 교차 검증을 통해 모델의 성능 평가 (예: MSE)\n",
        "    with parallel_backend(\"loky\", inner_max_num_threads=2):\n",
        "        scores = cross_val_score(model, X_train_filtered, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=2)\n",
        "    mse = -np.mean(scores)  # MSE 값이 음수이므로 양수로 변환\n",
        "\n",
        "    return mse,  # 튜플로 반환\n",
        "\n",
        "# Step 2: 유전 알고리즘 설정\n",
        "# 클래스가 이미 생성되었는지 확인\n",
        "if \"FitnessMin\" not in creator.__dict__:\n",
        "    creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))  # 최소화를 목표로 설정 (손실 최소화)\n",
        "\n",
        "if \"Individual\" not in creator.__dict__:\n",
        "    creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# 하이퍼파라미터 범위 정의 (유전 알고리즘에서 사용할 범위)\n",
        "toolbox.register(\"n_estimators\", random.randint, 50, 200)  # n_estimators 범위\n",
        "toolbox.register(\"max_depth\", random.randint, 10, 25)  # max_depth 범위\n",
        "toolbox.register(\"min_samples_split\", random.randint, 2, 10)  # min_samples_split 범위\n",
        "toolbox.register(\"min_samples_leaf\", random.randint, 1, 4)  # min_samples_leaf 범위 (1 이상의 값으로 설정)\n",
        "toolbox.register(\"max_features\", random.choice, ['sqrt', 'log2'])  # max_features 범위 (문자열)\n",
        "toolbox.register(\"min_weight_fraction_leaf\", random.uniform, 0.0, 0.5)  # min_weight_fraction_leaf 범위 (0.0에서 0.5 사이로 제한)\n",
        "\n",
        "# 개체(하이퍼파라미터 세트) 생성\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "                 (toolbox.n_estimators, toolbox.max_depth, toolbox.min_samples_split,\n",
        "                  toolbox.min_samples_leaf, toolbox.max_features, toolbox.min_weight_fraction_leaf))\n",
        "\n",
        "# 군집(집단) 생성\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# 평가 함수로 손실 함수 설정\n",
        "toolbox.register(\"evaluate\", rf_loss_function)\n",
        "\n",
        "# 유전 알고리즘 연산 정의 (교배, 변이, 선택)\n",
        "\n",
        "# 변이 연산 - 숫자형 파라미터에 대해 Gaussian 변이를 적용하고, 문자열 파라미터는 교체 방식 사용\n",
        "def custom_mutation(individual):\n",
        "    for i in range(len(individual)):\n",
        "        if isinstance(individual[i], (int, float)):\n",
        "            if random.random() < 0.2:  # 변이 확률\n",
        "                individual[i] += random.gauss(0, 1)\n",
        "                if i == 0:  # n_estimators\n",
        "                    individual[i] = max(1, individual[i])  # 1 이상 유지\n",
        "                elif i == 3:  # min_samples_leaf (1 이상 유지)\n",
        "                    individual[i] = max(1, int(individual[i]))\n",
        "                elif i == 5:  # min_weight_fraction_leaf\n",
        "                    individual[i] = max(0.0, min(0.5, individual[i]))  # 0.0~0.5 사이로 제한\n",
        "        elif isinstance(individual[i], str):\n",
        "            if random.random() < 0.2:  # 변이 확률\n",
        "                individual[i] = random.choice(['sqrt', 'log2'])  # 선택 교체\n",
        "    return individual,\n",
        "\n",
        "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)  # 교배 연산\n",
        "toolbox.register(\"mutate\", custom_mutation)  # 사용자 정의 변이 연산\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)  # 선택 연산\n",
        "\n",
        "# Step 3: 유전 알고리즘 실행\n",
        "def run_ga():\n",
        "    # 초기 집단 생성\n",
        "    population = toolbox.population(n=20)\n",
        "\n",
        "    # 유전자 알고리즘 실행 (세대 수, 교배/변이 확률 설정)\n",
        "    algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=30, verbose=True)\n",
        "\n",
        "    # 최적의 개체 찾기\n",
        "    best_individual = tools.selBest(population, k=1)[0]\n",
        "    print(\"Best individual (optimized hyperparameters):\", best_individual)\n",
        "    return best_individual\n",
        "\n",
        "# GA 실행\n",
        "best_hyperparameters = run_ga()\n",
        "\n",
        "# Step 4: 최적화된 하이퍼파라미터로 최종 Random Forest 모델 학습\n",
        "rf_optimized = RandomForestRegressor(\n",
        "    n_estimators=int(best_hyperparameters[0]),\n",
        "    max_depth=int(best_hyperparameters[1]) if best_hyperparameters[1] != None else None,\n",
        "    min_samples_split=int(best_hyperparameters[2]),\n",
        "    min_samples_leaf=int(best_hyperparameters[3]),\n",
        "    max_features=best_hyperparameters[4],\n",
        "    min_weight_fraction_leaf=best_hyperparameters[5],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_optimized.fit(X_train_filtered, y_train)\n",
        "\n",
        "# 최종 모델 평가\n",
        "y_pred_ga = rf_optimized.predict(X_test_filtered)\n",
        "\n",
        "# 성능 지표 계산\n",
        "mape_ga = np.mean(np.abs((y_test - y_pred_ga) / y_test)) * 100\n",
        "mae_ga = np.mean(np.abs(y_test - y_pred_ga))\n",
        "rmse_ga = np.sqrt(np.mean((y_test - y_pred_ga) ** 2))\n",
        "mse_ga = np.mean((y_test - y_pred_ga) ** 2)\n",
        "\n",
        "print(f\"MAPE (GA Optimized): {mape_ga:.4f}\")\n",
        "print(f\"MAE (GA Optimized): {mae_ga:.4f}\")\n",
        "print(f\"RMSE (GA Optimized): {rmse_ga:.4f}\")\n",
        "print(f\"MSE (GA Optimized): {mse_ga:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "DyOUQliyYHSh",
        "tokUWr_4YLQE"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}