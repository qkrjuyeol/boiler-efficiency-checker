{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HC9wodWCvMs"
      },
      "source": [
        "# 데이터 전처리 - 아스카로바"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxwPIiGo1C3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e65a04-911b-4831-9715-7e56ac787b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# 데이터 폴더 경로\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/3months_data'  # Update with the correct path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wQBlDt29Ypq",
        "outputId": "0378f382-744f-40cc-df06-74fe9c4c77ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File '/content/drive/MyDrive/3months_data/28_2024-07-25.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-07-26.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-07-27.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-07-28.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-07-29.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-07-30.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-07-31.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-08-01.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-08-02.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-08-06.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-08-07.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-08-08.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-08-09.csv' is empty and will be skipped.\n",
            "File '/content/drive/MyDrive/3months_data/28_2024-08-24.csv' is empty and will be skipped.\n"
          ]
        }
      ],
      "source": [
        "all_files = os.listdir(data_folder)\n",
        "csv_files = [os.path.join(data_folder, f) for f in all_files if f.endswith('.csv')]\n",
        "\n",
        "\n",
        "#가능한 모든 인코딩\n",
        "def read_csv_file(file_path):\n",
        "    encodings = ['utf-8', 'cp949', 'euc-kr']  # List of possible encodings\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            return pd.read_csv(file_path, encoding=encoding)\n",
        "        except (UnicodeDecodeError, pd.errors.EmptyDataError, pd.errors.ParserError):\n",
        "            continue\n",
        "    raise ValueError(f\"Could not read file {file_path} with any encoding.\")\n",
        "\n",
        "\n",
        "valid_dataframes = []\n",
        "#모든 파일을 포함한 dataframe 만들기\n",
        "for file in csv_files:\n",
        "    try:\n",
        "        df = read_csv_file(file)\n",
        "        if not df.empty:  # Check if the DataFrame has data\n",
        "            #print(f\"File '{file}' read successfully with {len(df)} rows.\")\n",
        "            valid_dataframes.append(df)\n",
        "        else:\n",
        "            print(f\"File '{file}' is empty and will be skipped.\")\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "#empty 파일 제외한 모든 파일을 dataframe으로 추가하기\n",
        "\n",
        "# Combine only the valid DataFrames\n",
        "if valid_dataframes:\n",
        "    combined_data = pd.concat(valid_dataframes, ignore_index=True)\n",
        "else:\n",
        "    combined_data = pd.DataFrame()  # Empty DataFrame if no valid files found\n",
        "\n",
        "# Verify the combined DataFrame\n",
        "#print(f\"Total rows in combined data: {len(combined_data)}\")\n",
        "#print(combined_data.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2FcwI5d8TtP"
      },
      "outputs": [],
      "source": [
        "combined_data.columns = [\n",
        "    \"생성일\", \"부하율\", \"설정 압력\",\n",
        "    \"보일러 압력\", \"송풍기 인버터 출력\", \"송풍기 입력\",\n",
        "    \"급수 펌프\", \"급수펌프 입력\", \"가스 댐퍼\",\n",
        "    \"가스 댐퍼 입력\", \"Air 댐퍼\", \"Air 댐퍼 입력\",\n",
        "    \"재순환 댐퍼\", \"재순환 외기 댐퍼\", \"재순환 댐퍼 입력\",\n",
        "    \"재순환 외기 댐퍼 입력\", \"급수 수위\", \"보일러 온도\",\n",
        "    \"배기가스온도1\", \"배기가스온도2\", \"배기가스온도3\",\n",
        "    \"배기 재 순환 온도\", \"에코 온도1\", \"에코 온도2\",\n",
        "    \"버너온도\", \"배기가스 NOx\", \"배기가스 O2\",\n",
        "    \"재순환 O2\", \"재순환 NOx\", \"급수량(적산유량)\",\n",
        "    \"급수량(순간유량)\", \"연료량(적산유량)\", \"연료량(순간유량)\",\n",
        "    \"효율(순간)\", \"소비전류\", \"진동센서1\",\n",
        "    \"진동센서2\", \"운전시간\", \"정상 운전 확률\",\n",
        "    \"송풍기 고장 확률\", \"AIR 댐퍼 고장 확률\", \"GAS 앰퍼 고장 확률\",\n",
        "    \"확률 업데이트 시간\", \"순간 스팀량\", \"입출력법 효율\",\n",
        "    \"열 손실법 효율\", \"효율(입출력법-스팀)\"\n",
        "]\n",
        "\n",
        "\n",
        "combined_data = combined_data.drop(columns=[\n",
        "    \"생성일\",\n",
        "    \"소비전류\",\n",
        "    \"진동센서1\",\n",
        "    \"진동센서2\",\n",
        "    \"운전시간\",\n",
        "    \"정상 운전 확률\",\n",
        "    \"송풍기 고장 확률\",\n",
        "    \"AIR 댐퍼 고장 확률\",\n",
        "    \"GAS 앰퍼 고장 확률\",\n",
        "    \"확률 업데이트 시간\",\n",
        "    \"순간 스팀량\",\n",
        "    \"입출력법 효율\",\n",
        "    \"열 손실법 효율\",\n",
        "    \"효율(입출력법-스팀)\",\n",
        "    \"배기 재 순환 온도\",\n",
        "    \"버너온도\"\n",
        "])\n",
        "\n",
        "\n",
        "#combined_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
        "#combined_data.fillna(method='bfill', inplace=True)  # Backward fill for any remaining NaNs\n",
        "\n",
        "scaler = StandardScaler()\n",
        "numeric_columns = combined_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "combined_data[numeric_columns] = scaler.fit_transform(combined_data[numeric_columns])\n",
        "\n",
        "# Select numerical columns only\n",
        "numerical_columns = combined_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Fill missing values in numerical columns with the mean value\n",
        "combined_data[numerical_columns] = combined_data[numerical_columns].apply(lambda col: col.fillna(col.mean()))\n",
        "\n",
        "# For categorical columns, fill missing values with the most frequent value\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "combined_data[combined_data.select_dtypes(include=['object']).columns] = cat_imputer.fit_transform(combined_data.select_dtypes(include=['object']))\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "label_encoder = LabelEncoder()\n",
        "for column in combined_data.select_dtypes(include=['object']).columns:\n",
        "    combined_data[column] = label_encoder.fit_transform(combined_data[column])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C2DdBCTIP9u"
      },
      "outputs": [],
      "source": [
        "combined_data.to_csv('/content/drive/MyDrive/preprocessed_boiler_data.csv', index=False, encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSWPh0oYC0HF"
      },
      "source": [
        "# XGBoost (RF 알고리즘으로 하이퍼 파라미터 튜닝) - 주열"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NZS__Ubht3i"
      },
      "source": [
        "## 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX7mK6jGCzTC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 타겟 변수와 특징 변수 분리\n",
        "X = df.drop(columns=['효율(순간)'])  # 타겟 변수를 제외한 특징 변수\n",
        "y = df['효율(순간)']  # 타겟 변수\n",
        "\n",
        "# 범주형 변수를 One-Hot Encoding으로 변환\n",
        "X_encoded = pd.get_dummies(X)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWC8JMSNqTg4",
        "outputId": "0666f9ef-4b8e-45fe-f822-7e477eda02c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB93W4taC5df",
        "outputId": "e62e34d8-7d6f-4a80-f9fd-b745c4daf903"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최적의 하이퍼파라미터: {'subsample': 0.8999999999999999, 'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.2, 'colsample_bytree': 0.5}\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# XGBoost 모델 초기화\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# 하이퍼파라미터 범위 설정\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(50, 500, 50),\n",
        "    'max_depth': np.arange(3, 10),\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': np.arange(0.5, 1.0, 0.1),\n",
        "    'colsample_bytree': np.arange(0.5, 1.0, 0.1)\n",
        "}\n",
        "\n",
        "# 랜덤 서치 설정\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=100,  # 시도할 조합의 수\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,  # 5겹 교차검증\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # 모든 CPU 코어 사용\n",
        ")\n",
        "\n",
        "# 랜덤 서치 실행\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 파라미터 출력\n",
        "print(\"최적의 하이퍼파라미터:\", random_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MaGzekHC7tR",
        "outputId": "856ddee9-4726-4cff-81dc-871bf084ccda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 데이터 MSE: 0.20851213871603325\n",
            "테스트 데이터 MAE: 0.10359154530361854\n",
            "테스트 데이터 RMSE: 0.4566312940612297\n",
            "테스트 데이터 MAPE: 6.04%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np  # numpy를 임포트하여 sqrt 사용\n",
        "\n",
        "# 예측\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# 성능 평가\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)  # RMSE 계산\n",
        "\n",
        "# MAPE 계산\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # MAPE 계산 (백분율로)\n",
        "\n",
        "# 결과 출력\n",
        "print(f'테스트 데이터 MSE: {mse}')\n",
        "print(f'테스트 데이터 MAE: {mae}')\n",
        "print(f'테스트 데이터 RMSE: {rmse}')\n",
        "print(f'테스트 데이터 MAPE: {mape:.2f}%')  # MAPE를 소수점 2자리로 포맷\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjMX5bmRhf8E"
      },
      "source": [
        "## 데이터 시각화"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y fonts-nanum\n",
        "!fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLpL6MEjEvC0",
        "outputId": "817717d9-37d0-4e13-a208-76dd5c4dbd72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 0s (23.2 MB/s)\n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 119626 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/share/fonts/truetype/nanum/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5WP7t52Fi8d",
        "outputId": "50c9974b-c677-465f-e67f-da0c31fc36c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NanumBarunGothicBold.ttf  NanumGothicCodingBold.ttf  NanumMyeongjoBold.ttf  NanumSquareRoundB.ttf\n",
            "NanumBarunGothic.ttf\t  NanumGothicCoding.ttf      NanumMyeongjo.ttf\t    NanumSquareRoundR.ttf\n",
            "NanumGothicBold.ttf\t  NanumGothic.ttf\t     NanumSquareB.ttf\t    NanumSquareR.ttf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fc-list :lang=ko\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAhNqkKKFB-A",
        "outputId": "aa240635-dc82-4130-e300-b307374feb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/share/fonts/truetype/nanum/NanumSquareRoundB.ttf: NanumSquareRound,나눔스퀘어라운드,NanumSquareRound Bold,나눔스퀘어라운드 Bold:style=Bold,Regular\n",
            "/usr/share/fonts/truetype/nanum/NanumGothicCodingBold.ttf: NanumGothicCoding,나눔고딕코딩:style=Bold\n",
            "/usr/share/fonts/truetype/nanum/NanumSquareRoundR.ttf: NanumSquareRound,나눔스퀘어라운드,NanumSquareRound Regular,나눔스퀘어라운드 Regular:style=Regular\n",
            "/usr/share/fonts/truetype/nanum/NanumSquareB.ttf: NanumSquare,나눔스퀘어,NanumSquare Bold,나눔스퀘어 Bold:style=Bold\n",
            "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf: NanumBarunGothic,나눔바른고딕:style=Regular\n",
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf: NanumGothic,나눔고딕:style=Regular\n",
            "/usr/share/fonts/truetype/nanum/NanumGothicCoding.ttf: NanumGothicCoding,나눔고딕코딩:style=Regular\n",
            "/usr/share/fonts/truetype/nanum/NanumBarunGothicBold.ttf: NanumBarunGothic,나눔바른고딕:style=Bold\n",
            "/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf: NanumGothic,나눔고딕:style=Bold\n",
            "/usr/share/fonts/truetype/nanum/NanumSquareR.ttf: NanumSquare,나눔스퀘어:style=Regular\n",
            "/usr/share/fonts/truetype/nanum/NanumMyeongjo.ttf: NanumMyeongjo,나눔명조:style=Regular\n",
            "/usr/share/fonts/truetype/nanum/NanumMyeongjoBold.ttf: NanumMyeongjo,나눔명조:style=Bold\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "collapsed": true,
        "id": "A1STuN0jhfrb",
        "outputId": "d56ca022-4c58-45b1-bb2d-70f914710387"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "All arrays must be of the same length",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9652a0fc36f9>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 특성 이름과 중요도를 DataFrame으로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m importance_df = pd.DataFrame({\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;34m'Feature'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m'Importance'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# Manually set the path to NanumMyeongjo font (which appears to be installed)\n",
        "font_path = '/content/NanumGothic-Bold.ttf'  # Path from fc-list\n",
        "fontprop = fm.FontProperties(fname=font_path)\n",
        "plt.rcParams['font.family'] = fontprop.get_name()\n",
        "\n",
        "# Avoid breaking of negative signs\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# XGBoost 모델에서 특성 중요도 가져오기\n",
        "feature_importances = random_search.best_estimator_.feature_importances_\n",
        "\n",
        "# 특성 이름과 중요도를 DataFrame으로 변환\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# 특성 중요도 시각화\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
        "plt.gca().invert_yaxis()  # 중요도가 높은 순서부터 표시\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance for Efficiency (Instantaneous)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxq89UlJ92Np"
      },
      "source": [
        "# XGBoost (RF 알고리즘으로 하이퍼 파라미터 튜닝) -희성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IB2SvUNk-BKl",
        "outputId": "e5f4e513-27e9-48a1-9d83-c6be8558c4a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 500 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n500 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1081, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 1573, in __init__\n    self._init(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 1632, in _init\n    it.reraise()\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 569, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 550, in _handle_exception\n    return fn()\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 637, in <lambda>\n    return self._handle_exception(lambda: self.next(input_data), 0)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 1416, in next\n    input_data(**self.kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 617, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 1459, in _proxy_transform\n    df, feature_names, feature_types = _transform_pandas_df(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 603, in _transform_pandas_df\n    pandas_check_dtypes(data, enable_categorical)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 569, in pandas_check_dtypes\n    _invalid_dataframe_dtype(data)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 356, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:배기가스온도3: object, 에코 온도2: object\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d9eb363a0dc1>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# 랜덤 서치 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 훈련 데이터를 사용하여 랜덤 서치 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# 최적의 하이퍼파라미터 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1961\u001b[0m             ParameterSampler(\n\u001b[1;32m   1962\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    994\u001b[0m                     )\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             )\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 500 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n500 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1081, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 596, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1003, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 1573, in __init__\n    self._init(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 1632, in _init\n    it.reraise()\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 569, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 550, in _handle_exception\n    return fn()\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 637, in <lambda>\n    return self._handle_exception(lambda: self.next(input_data), 0)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 1416, in next\n    input_data(**self.kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 617, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 1459, in _proxy_transform\n    df, feature_names, feature_types = _transform_pandas_df(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 603, in _transform_pandas_df\n    pandas_check_dtypes(data, enable_categorical)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 569, in pandas_check_dtypes\n    _invalid_dataframe_dtype(data)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 356, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:배기가스온도3: object, 에코 온도2: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd  # 데이터 처리에 사용되는 pandas 라이브러리 임포트\n",
        "import numpy as np  # 수치 계산을 위한 numpy 라이브러리 임포트\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV  # 데이터 분리 및 하이퍼파라미터 튜닝을 위한 모듈 임포트\n",
        "import xgboost as xgb  # XGBoost 모델을 사용하기 위한 라이브러리 임포트\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error  # 회귀 모델 평가를 위한 메트릭 임포트\n",
        "import matplotlib.pyplot as plt  # 데이터 시각화를 위한 matplotlib 임포트\n",
        "import seaborn as sns  # 데이터 시각화를 위한 seaborn 임포트\n",
        "\n",
        "# 데이터 불러오기\n",
        "combined_data = pd.read_csv('/content/drive/MyDrive/preprocessed_boiler_data.csv')  # 전처리된 CSV 파일을 읽어와 DataFrame으로 저장\n",
        "\n",
        "# 특성과 타겟 변수 분리\n",
        "X = combined_data.drop(columns=['효율(순간)'])  # 타겟 변수를 제외한 특징 변수로 X 설정\n",
        "y = combined_data['효율(순간)']  # 타겟 변수로 '효율(순간)' 열 설정\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 데이터를 80% 훈련 세트, 20% 테스트 세트로 분리\n",
        "\n",
        "# XGBoost 모델 초기화\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')  # 회귀 모델을 위한 XGBoost 초기화\n",
        "\n",
        "# 하이퍼파라미터 범위 설정\n",
        "param_dist = {  # 하이퍼파라미터 범위 정의\n",
        "    'n_estimators': np.arange(50, 500, 50),  # 트리의 개수\n",
        "    'max_depth': np.arange(3, 10),  # 트리의 최대 깊이\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # 학습률\n",
        "    'subsample': np.arange(0.5, 1.0, 0.1),  # 각 트리가 학습할 데이터 비율\n",
        "    'colsample_bytree': np.arange(0.5, 1.0, 0.1)  # 각 트리가 사용할 피처 비율\n",
        "}\n",
        "\n",
        "# 랜덤 서치 설정\n",
        "random_search = RandomizedSearchCV(  # 랜덤 서치 객체 생성\n",
        "    xgb_model,  # 사용할 모델\n",
        "    param_distributions=param_dist,  # 하이퍼파라미터 범위\n",
        "    n_iter=100,  # 시도할 조합의 수\n",
        "    scoring='neg_mean_squared_error',  # 평가 지표로 음의 평균 제곱 오차 사용\n",
        "    cv=5,  # 5겹 교차 검증\n",
        "    verbose=1,  # 진행 상황 출력\n",
        "    random_state=42,  # 난수 시드 설정\n",
        "    n_jobs=-1  # 모든 CPU 코어 사용\n",
        ")\n",
        "\n",
        "# 랜덤 서치 실행\n",
        "random_search.fit(X_train, y_train)  # 훈련 데이터를 사용하여 랜덤 서치 실행\n",
        "\n",
        "# 최적의 하이퍼파라미터 출력\n",
        "print(\"최적의 하이퍼파라미터:\", random_search.best_params_)  # 찾은 최적의 하이퍼파라미터 출력\n",
        "\n",
        "# 예측\n",
        "y_pred = random_search.predict(X_test)  # 테스트 데이터를 사용하여 예측 수행\n",
        "\n",
        "# 성능 평가\n",
        "mse = mean_squared_error(y_test, y_pred)  # 평균 제곱 오차 계산\n",
        "mae = mean_absolute_error(y_test, y_pred)  # 평균 절대 오차 계산\n",
        "rmse = np.sqrt(mse)  # 제곱근 평균 제곱 오차 계산\n",
        "\n",
        "# MAPE 계산\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # 평균 절대 비율 오차 계산 (백분율로)\n",
        "\n",
        "# 결과 출력\n",
        "print(f'테스트 데이터 MSE: {mse}')  # 테스트 데이터의 MSE 출력\n",
        "print(f'테스트 데이터 MAE: {mae}')  # 테스트 데이터의 MAE 출력\n",
        "print(f'테스트 데이터 RMSE: {rmse}')  # 테스트 데이터의 RMSE 출력\n",
        "print(f'테스트 데이터 MAPE: {mape:.2f}%')  # 테스트 데이터의 MAPE 출력 (소수점 2자리로 포맷)\n",
        "\n",
        "# 상관 행렬 시각화\n",
        "plt.figure(figsize=(12, 8))  # 그림 크기 설정\n",
        "correlation_matrix = combined_data.corr()  # 상관 행렬 계산\n",
        "sns.heatmap(correlation_matrix[['효율(순간)']].sort_values(by='효율(순간)', ascending=False),  # '효율(순간)'과의 상관 관계 시각화\n",
        "            annot=True, cmap='coolwarm', vmin=-1, vmax=1)  # 각 셀에 값 표시, 색상 맵 설정\n",
        "plt.title('Correlation with Efficiency (Instantaneous)')  # 제목 설정\n",
        "plt.show()  # 그래프 출력\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "nxq89UlJ92Np"
      ],
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}