{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "데이터 전처리"
      ],
      "metadata": {
        "id": "SPelJ0GOIc0h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyzR-VCdIHW9",
        "outputId": "11e2222b-efca-4f8d-d54e-5043ef072756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/2months_data/2months_data'\n",
        "all_files = os.listdir(data_folder)\n",
        "csv_files = [os.path.join(data_folder, f) for f in all_files if f.endswith('.csv')]\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    encodings = ['utf-8', 'cp949', 'euc-kr']  # List of possible encodings\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            return pd.read_csv(file_path, encoding=encoding)\n",
        "        except (UnicodeDecodeError, pd.errors.EmptyDataError, pd.errors.ParserError):\n",
        "            continue\n",
        "    raise ValueError(f\"Could not read file {file_path} with any encoding.\")\n",
        "\n",
        "valid_dataframes = []\n",
        "for file in csv_files:\n",
        "    try:\n",
        "        df = read_csv_file(file)\n",
        "        if not df.empty:\n",
        "          valid_dataframes.append(df)\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "\n",
        "if valid_dataframes:\n",
        "    combined_data = pd.concat(valid_dataframes, ignore_index=True)\n",
        "else:\n",
        "    combined_data = pd.DataFrame()\n",
        "combined_data.columns=[\"Creation date\", \"load factor\", \"Set Pressure\", \"Boiler Pressure\",\n",
        "                      \"Blower Inverter Output\", \"Blower Input\", \"Water Supply Pump\",\n",
        "                      \"Water Supply Pump Input\", \"Gas Damper\", \"Gas Damper Input\",\n",
        "                      \"Air Damper\", \"Air Damper Input\", \"Recirculation Damper\",\n",
        "                      \"Recirculation External Damper\", \"Recirculation Damper Input\",\n",
        "                      \"Recirculation External Damper Input\", \"Water Supply Level\",\n",
        "                      \"Boiler Temperature\", \"Exhaust Gas Temperature 1\",\n",
        "                      \"Exhaust Gas Temperature 2\", \"Exhaust Gas Temperature 3\",\n",
        "                      \"Exhaust Recirculation Temperature\", \"Economizer Temperature 1\",\n",
        "                      \"Economizer Temperature 2\", \"Burner Temperature\", \"Exhaust Gas NOx\",\n",
        "                      \"Exhaust Gas O2\", \"Recirculation O2\", \"Recirculation NOx\",\n",
        "                      \"Water Supply Amount (Cumulative Flow)\",\n",
        "                      \"Water Supply Amount (Instantaneous Flow)\",\n",
        "                      \"Fuel Amount (Cumulative Flow)\", \"Fuel Amount (Instantaneous Flow)\",\n",
        "                      \"Efficiency (Instantaneous)\", \"Power Consumption\", \"Vibration Sensor 1\",\n",
        "                      \"Vibration Sensor 2\", \"Operating Time\", \"Normal Operation Probability\",\n",
        "                      \"Blower Failure Probability\", \"Air Damper Failure Probability\",\n",
        "                      \"Gas Damper Failure Probability\", \"Probability Update Time\",\n",
        "                      \"Instantaneous Steam Amount\", \"Input-Output Efficiency\",\n",
        "                      \"Heat Loss Efficiency\", \"Efficiency (input/output method-steam)\"]\n",
        "combined_data = combined_data.drop(columns=[\n",
        "    \"Creation date\",\"Power Consumption\",\"Vibration Sensor 1\",\"Vibration Sensor 2\",\"Operating Time\",\n",
        "    \"Normal Operation Probability\",\"Blower Failure Probability\",\"Air Damper Failure Probability\",\n",
        "    \"Gas Damper Failure Probability\",\"Probability Update Time\",\"Instantaneous Steam Amount\",\n",
        "    \"Input-Output Efficiency\",\"Heat Loss Efficiency\",\"Efficiency (input/output method-steam)\",\n",
        "    \"Exhaust Recirculation Temperature\",\"Burner Temperature\"\n",
        "])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "numeric_columns = combined_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "combined_data[numeric_columns] = scaler.fit_transform(combined_data[numeric_columns])\n",
        "\n",
        "# Select numerical columns only\n",
        "numerical_columns = combined_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Fill missing values in numerical columns with the mean value\n",
        "combined_data[numerical_columns] = combined_data[numerical_columns].apply(lambda col: col.fillna(col.mean()))\n",
        "\n",
        "# For categorical columns, fill missing values with the most frequent value\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "combined_data[combined_data.select_dtypes(include=['object']).columns] = cat_imputer.fit_transform(combined_data.select_dtypes(include=['object']))\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "label_encoder = LabelEncoder()\n",
        "for column in combined_data.select_dtypes(include=['object']).columns:\n",
        "    combined_data[column] = label_encoder.fit_transform(combined_data[column])\n",
        "\n",
        "combined_data.to_csv('/content/drive/MyDrive/preprocessed_boiler_data.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "랜덤 포레스트\n",
        "랜덤 서치로 하이퍼파라미터 찾기"
      ],
      "metadata": {
        "id": "igJ7z4p4I_F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.stats import randint\n",
        "\n",
        "# combined_data = 전처리된 데이터프레임 (전처리 코드는 이미 실행된 상태라고 가정)\n",
        "\n",
        "# 독립 변수(X)와 종속 변수(y) 설정\n",
        "X = combined_data.drop(columns=['Efficiency (Instantaneous)'])  # 종속 변수 제외\n",
        "y = combined_data['Efficiency (Instantaneous)']\n",
        "\n",
        "# 데이터 분할: 훈련 데이터와 테스트 데이터로 분할\n",
        "# 여기에서 train_test_split을 사용하여 X_train, X_test, y_train, y_test를 생성\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 예시: 80% 훈련, 20% 테스트\n",
        "\n",
        "# Random Forest Model with RandomizedSearchCV for hyperparameter tuning\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100, 200],      # Number of trees\n",
        "    'max_depth': [None, 10, 15, 20, 25],           # Maximum depth of the tree\n",
        "    'min_samples_split': [2,5,10],   # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1,2,4],    # Minimum number of samples required to be at a leaf node\n",
        "    'max_features':['sqrt', 'log2'],  # Number of features to consider at each split\n",
        "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2]  # 리프 노드의 가중치 샘플 최소값\n",
        "}\n",
        "\n",
        "# Randomized Search CV with 5-fold cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions,\n",
        "                                   n_iter=50, cv=5, verbose=2, n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best hyperparameters found by RandomizedSearchCV:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# Model evaluation using test data\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "mae = np.mean(np.abs(y_test - y_pred))\n",
        "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
        "mse = np.mean((y_test - y_pred) ** 2)\n",
        "\n",
        "print(f\"MAPE: {mape:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYib_M3_IkvD",
        "outputId": "8768df14-fc90-44c3-8b61-8f7afe5072d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters found by RandomizedSearchCV:\n",
            "{'n_estimators': 50, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 25}\n",
            "MAPE: 7.8880\n",
            "MAE: 0.0207\n",
            "RMSE: 0.0472\n",
            "MSE: 0.0022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼파라미터 값 집어넣고, 특정 인자값만 추출해서 다시 랜덤포레스트"
      ],
      "metadata": {
        "id": "LENy5c0pJb7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "# 전처리 완료된 데이터 로드\n",
        "data_folder = '/content/drive/MyDrive/2months_data/2months_data'\n",
        "all_files = os.listdir(data_folder)\n",
        "csv_files = [os.path.join(data_folder, f) for f in all_files if f.endswith('.csv')]\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    encodings = ['utf-8', 'cp949', 'euc-kr']  # List of possible encodings\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            return pd.read_csv(file_path, encoding=encoding)\n",
        "        except (UnicodeDecodeError, pd.errors.EmptyDataError, pd.errors.ParserError):\n",
        "            continue\n",
        "    raise ValueError(f\"Could not read file {file_path} with any encoding.\")\n",
        "\n",
        "valid_dataframes = []\n",
        "for file in csv_files:\n",
        "    try:\n",
        "        df = read_csv_file(file)\n",
        "        if not df.empty:\n",
        "          valid_dataframes.append(df)\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "\n",
        "if valid_dataframes:\n",
        "    combined_data = pd.concat(valid_dataframes, ignore_index=True)\n",
        "else:\n",
        "    combined_data = pd.DataFrame()\n",
        "combined_data.columns=[\"Creation date\", \"load factor\", \"Set Pressure\", \"Boiler Pressure\",\n",
        "                      \"Blower Inverter Output\", \"Blower Input\", \"Water Supply Pump\",\n",
        "                      \"Water Supply Pump Input\", \"Gas Damper\", \"Gas Damper Input\",\n",
        "                      \"Air Damper\", \"Air Damper Input\", \"Recirculation Damper\",\n",
        "                      \"Recirculation External Damper\", \"Recirculation Damper Input\",\n",
        "                      \"Recirculation External Damper Input\", \"Water Supply Level\",\n",
        "                      \"Boiler Temperature\", \"Exhaust Gas Temperature 1\",\n",
        "                      \"Exhaust Gas Temperature 2\", \"Exhaust Gas Temperature 3\",\n",
        "                      \"Exhaust Recirculation Temperature\", \"Economizer Temperature 1\",\n",
        "                      \"Economizer Temperature 2\", \"Burner Temperature\", \"Exhaust Gas NOx\",\n",
        "                      \"Exhaust Gas O2\", \"Recirculation O2\", \"Recirculation NOx\",\n",
        "                      \"Water Supply Amount (Cumulative Flow)\",\n",
        "                      \"Water Supply Amount (Instantaneous Flow)\",\n",
        "                      \"Fuel Amount (Cumulative Flow)\", \"Fuel Amount (Instantaneous Flow)\",\n",
        "                      \"Efficiency (Instantaneous)\", \"Power Consumption\", \"Vibration Sensor 1\",\n",
        "                      \"Vibration Sensor 2\", \"Operating Time\", \"Normal Operation Probability\",\n",
        "                      \"Blower Failure Probability\", \"Air Damper Failure Probability\",\n",
        "                      \"Gas Damper Failure Probability\", \"Probability Update Time\",\n",
        "                      \"Instantaneous Steam Amount\", \"Input-Output Efficiency\",\n",
        "                      \"Heat Loss Efficiency\", \"Efficiency (input/output method-steam)\"]\n",
        "\n",
        "combined_data = combined_data.drop(columns=[\n",
        "    \"Creation date\",\"Power Consumption\",\"Vibration Sensor 1\",\"Vibration Sensor 2\",\"Operating Time\",\n",
        "    \"Normal Operation Probability\",\"Blower Failure Probability\",\"Air Damper Failure Probability\",\n",
        "    \"Gas Damper Failure Probability\",\"Probability Update Time\",\"Instantaneous Steam Amount\",\n",
        "    \"Input-Output Efficiency\",\"Heat Loss Efficiency\",\"Efficiency (input/output method-steam)\",\n",
        "    \"Exhaust Recirculation Temperature\",\"Burner Temperature\"\n",
        "])\n",
        "\n",
        "combined_data = combined_data.replace('-', np.nan)\n",
        "combined_data = combined_data.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "\n",
        "# 독립 변수(X)와 종속 변수(y) 설정\n",
        "X = combined_data.drop(columns=['Efficiency (Instantaneous)'])\n",
        "y = combined_data['Efficiency (Instantaneous)']\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 최적의 하이퍼파라미터를 반영한 랜덤 포레스트 모델 재학습\n",
        "rf_optimized = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    min_weight_fraction_leaf=0.0,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    max_depth=25,\n",
        "    random_state=42\n",
        ")\n",
        "rf_optimized.fit(X_train, y_train)\n",
        "\n",
        "# 랜덤 포레스트 모델에서 피처 중요도 상위 16개 선택\n",
        "importances = rf_optimized.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:16]\n",
        "X_selected = X.iloc[:, indices]\n",
        "\n",
        "# 선택한 피처를 사용해 랜덤 포레스트 모델 재학습\n",
        "X_train_selected, X_test_selected, y_train_selected, y_test_selected = train_test_split(\n",
        "    X_selected, y, test_size=0.2, random_state=42\n",
        ")\n",
        "rf_optimized.fit(X_train_selected, y_train_selected)\n",
        "\n",
        "# 랜덤 포레스트 모델 예측을 손실함수로 활용하는 딥러닝 모델 정의\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_selected.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# 최적화 모델 설정\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss=lambda y_true, y_pred: tf.reduce_mean(tf.square(rf_optimized.predict(X_test_selected) - y_pred)), metrics=['mae'])\n",
        "\n",
        "# 미니배치 경사하강법으로 학습\n",
        "history = model.fit(X_train_selected, y_train_selected, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 최적화된 보일러 효율 평가\n",
        "loss, mae = model.evaluate(X_test_selected, y_test_selected, verbose=0)\n",
        "y_pred = model.predict(X_test_selected)\n",
        "\n",
        "# 성능 지표 계산 및 결과 출력\n",
        "mape = mean_absolute_percentage_error(y_test_selected, y_pred)\n",
        "mae_final = mean_absolute_error(y_test_selected, y_pred)\n",
        "mse_final = mean_squared_error(y_test_selected, y_pred)\n",
        "\n",
        "print(f\"최적화된 보일러 효율 (MAE): {mae_final:.4f}\")\n",
        "print(f\"최종 손실값 (MSE): {mse_final:.4f}\")\n",
        "print(f\"MAPE: {mape:.4f}\")\n",
        "\n",
        "# 사용된 인자값 세트를 표로 출력\n",
        "optimized_features = X_selected.columns[indices]\n",
        "feature_importance_table = pd.DataFrame({\n",
        "    'Feature': optimized_features,\n",
        "    'Importance': importances[indices]\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "print(\"\\n효율값 계산에 사용된 인자 세트:\")\n",
        "print(feature_importance_table)\n",
        "\n",
        "# 최적화 과정 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "fcb3jKoyJPjr",
        "outputId": "6b10df56-dd3b-4c0a-d085-d41651e4fab9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-379ad1e2b379>:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  combined_data = combined_data.replace('-', np.nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "56008/56008 [==============================] - 2958s 52ms/step - loss: 227249.5469 - mae: 118.1224 - val_loss: 437.8788 - val_mae: 18.7494\n",
            "Epoch 2/50\n",
            "  502/56008 [..............................] - ETA: 45:30 - loss: 6645.4795 - mae: 52.4989"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-379ad1e2b379>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;31m# 미니배치 경사하강법으로 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# 최적화된 보일러 효율 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}