{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qkrjuyeol/boiler-efficiency-checker/blob/main/boiler-efficiency-checker/AI/RandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBpE3ijHa7K6"
      },
      "source": [
        "# RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr-Rnq4ZB1rc",
        "outputId": "e3b23c1e-63cd-4746-b70d-adc50e5e3f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/2months_data'\n",
        "all_files = os.listdir(data_folder)\n",
        "csv_files = [os.path.join(data_folder, f) for f in all_files if f.endswith('.csv')]\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    encodings = ['utf-8', 'cp949', 'euc-kr']  # List of possible encodings\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            return pd.read_csv(file_path, encoding=encoding)\n",
        "        except (UnicodeDecodeError, pd.errors.EmptyDataError, pd.errors.ParserError):\n",
        "            continue\n",
        "    raise ValueError(f\"Could not read file {file_path} with any encoding.\")\n",
        "\n",
        "valid_dataframes = []\n",
        "for file in csv_files:\n",
        "    try:\n",
        "        df = read_csv_file(file)\n",
        "        if not df.empty:\n",
        "          valid_dataframes.append(df)\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "\n",
        "if valid_dataframes:\n",
        "    combined_data = pd.concat(valid_dataframes, ignore_index=True)\n",
        "else:\n",
        "    combined_data = pd.DataFrame()\n",
        "combined_data.columns=[\"Creation date\", \"load factor\", \"Set Pressure\", \"Boiler Pressure\",\n",
        "                      \"Blower Inverter Output\", \"Blower Input\", \"Water Supply Pump\",\n",
        "                      \"Water Supply Pump Input\", \"Gas Damper\", \"Gas Damper Input\",\n",
        "                      \"Air Damper\", \"Air Damper Input\", \"Recirculation Damper\",\n",
        "                      \"Recirculation External Damper\", \"Recirculation Damper Input\",\n",
        "                      \"Recirculation External Damper Input\", \"Water Supply Level\",\n",
        "                      \"Boiler Temperature\", \"Exhaust Gas Temperature 1\",\n",
        "                      \"Exhaust Gas Temperature 2\", \"Exhaust Gas Temperature 3\",\n",
        "                      \"Exhaust Recirculation Temperature\", \"Economizer Temperature 1\",\n",
        "                      \"Economizer Temperature 2\", \"Burner Temperature\", \"Exhaust Gas NOx\",\n",
        "                      \"Exhaust Gas O2\", \"Recirculation O2\", \"Recirculation NOx\",\n",
        "                      \"Water Supply Amount (Cumulative Flow)\",\n",
        "                      \"Water Supply Amount (Instantaneous Flow)\",\n",
        "                      \"Fuel Amount (Cumulative Flow)\", \"Fuel Amount (Instantaneous Flow)\",\n",
        "                      \"Efficiency (Instantaneous)\", \"Power Consumption\", \"Vibration Sensor 1\",\n",
        "                      \"Vibration Sensor 2\", \"Operating Time\", \"Normal Operation Probability\",\n",
        "                      \"Blower Failure Probability\", \"Air Damper Failure Probability\",\n",
        "                      \"Gas Damper Failure Probability\", \"Probability Update Time\",\n",
        "                      \"Instantaneous Steam Amount\", \"Input-Output Efficiency\",\n",
        "                      \"Heat Loss Efficiency\", \"Efficiency (input/output method-steam)\"]\n",
        "combined_data = combined_data.drop(columns=[\n",
        "    \"Creation date\",\"Power Consumption\",\"Vibration Sensor 1\",\"Vibration Sensor 2\",\"Operating Time\",\n",
        "    \"Normal Operation Probability\",\"Blower Failure Probability\",\"Air Damper Failure Probability\",\n",
        "    \"Gas Damper Failure Probability\",\"Probability Update Time\",\"Instantaneous Steam Amount\",\n",
        "    \"Input-Output Efficiency\",\"Heat Loss Efficiency\",\"Efficiency (input/output method-steam)\",\n",
        "    \"Exhaust Recirculation Temperature\",\"Burner Temperature\"\n",
        "])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "numeric_columns = combined_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "combined_data[numeric_columns] = scaler.fit_transform(combined_data[numeric_columns])\n",
        "\n",
        "# Select numerical columns only\n",
        "numerical_columns = combined_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Fill missing values in numerical columns with the mean value\n",
        "combined_data[numerical_columns] = combined_data[numerical_columns].apply(lambda col: col.fillna(col.mean()))\n",
        "\n",
        "# For categorical columns, fill missing values with the most frequent value\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "combined_data[combined_data.select_dtypes(include=['object']).columns] = cat_imputer.fit_transform(combined_data.select_dtypes(include=['object']))\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "label_encoder = LabelEncoder()\n",
        "for column in combined_data.select_dtypes(include=['object']).columns:\n",
        "    combined_data[column] = label_encoder.fit_transform(combined_data[column])\n",
        "\n",
        "combined_data.to_csv('/content/drive/MyDrive/preprocessed_boiler_data.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V9eUN2oB8jh"
      },
      "outputs": [],
      "source": [
        "combined_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg5ir7ndB-hC",
        "outputId": "fc01deb5-300b-43bb-b463-b4fc81bb8122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters found by RandomizedSearchCV:\n",
            "{'n_estimators': 50, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 25}\n",
            "MAPE: 7.8843\n",
            "MAE: 0.0207\n",
            "RMSE: 0.0475\n",
            "MSE: 0.0023\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.stats import randint\n",
        "\n",
        "# combined_data = 전처리된 데이터프레임 (전처리 코드는 이미 실행된 상태라고 가정)\n",
        "\n",
        "# 독립 변수(X)와 종속 변수(y) 설정\n",
        "X = combined_data.drop(columns=['Efficiency (Instantaneous)'])  # 종속 변수 제외\n",
        "y = combined_data['Efficiency (Instantaneous)']\n",
        "\n",
        "# 데이터 분할: 훈련 데이터와 테스트 데이터로 분할\n",
        "# 여기에서 train_test_split을 사용하여 X_train, X_test, y_train, y_test를 생성\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 예시: 80% 훈련, 20% 테스트\n",
        "\n",
        "# Random Forest Model with RandomizedSearchCV for hyperparameter tuning\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100, 200],      # Number of trees\n",
        "    'max_depth': [None, 10, 15, 20, 25],           # Maximum depth of the tree\n",
        "    'min_samples_split': [2,5,10],   # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1,2,4],    # Minimum number of samples required to be at a leaf node\n",
        "    'max_features':['sqrt', 'log2'],  # Number of features to consider at each split\n",
        "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2]  # 리프 노드의 가중치 샘플 최소값\n",
        "}\n",
        "\n",
        "# Randomized Search CV with 5-fold cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions,\n",
        "                                   n_iter=50, cv=5, verbose=2, n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best hyperparameters found by RandomizedSearchCV:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# Model evaluation using test data\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "mae = np.mean(np.abs(y_test - y_pred))\n",
        "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
        "mse = np.mean((y_test - y_pred) ** 2)\n",
        "\n",
        "print(f\"MAPE: {mape:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcxkM0K178zE"
      },
      "source": [
        "# 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ0Mu7SyCAvv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.font_manager as fm\n",
        "import shutil\n",
        "\n",
        "# Seaborn 스타일 적용\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# 예시 데이터 (random_search.best_estimator_는 미리 학습된 상태여야 함)\n",
        "feature_importances = random_search.best_estimator_.feature_importances_\n",
        "features = X.columns  # 각 피처의 이름\n",
        "\n",
        "# 피처 중요도를 시리즈로 변환하고, 중요도 순서대로 정렬\n",
        "feature_importance_series = pd.Series(feature_importances, index=features).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(x=feature_importance_series, y=feature_importance_series.index)\n",
        "\n",
        "# 각 바에 중요도 값을 텍스트로 표시\n",
        "for i, (value, name) in enumerate(zip(feature_importance_series, feature_importance_series.index)):\n",
        "    ax.text(value, i, f'{value:.4f}', va='center', ha='left', fontsize=10, color='black')\n",
        "\n",
        "plt.title('Feature Importance from Random Forest Model (with values)', fontsize=14)\n",
        "plt.xlabel('Importance Score', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1kovRVACDz6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRa7Uln_bAcP"
      },
      "source": [
        "# GA - 주열"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U5OyNQDz6IJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba348cd-a60b-4e60-d294-6b259c72e711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.26.4)\n",
            "Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deap\n",
            "Successfully installed deap-1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install deap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX4Yth_X8jns"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "X7InqOeWbmMa",
        "outputId": "44a6bb54-acd9-4baa-9f84-3f190be3b82a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gen\tnevals\n",
            "0  \t20    \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-23e91c5586da>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mmutpb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m  \u001b[0;31m# 변이 확률\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mresult_population\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meaSimple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcxpb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutpb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# 최적의 하이퍼파라미터 찾기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deap/algorithms.py\u001b[0m in \u001b[0;36meaSimple\u001b[0;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0minvalid_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moffspring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-23e91c5586da>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(individual)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     )\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         tree._fit(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from deap import base, creator, tools, algorithms\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
        "\n",
        "# 데이터 준비\n",
        "X = combined_data.drop(columns=['Efficiency (Instantaneous)'])\n",
        "y = combined_data['Efficiency (Instantaneous)']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# RandomizedSearchCV에서 얻은 최적의 하이퍼파라미터\n",
        "best_params_from_random_search = {\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 15,\n",
        "    'min_samples_split': 5,\n",
        "    'min_samples_leaf': 2\n",
        "}\n",
        "\n",
        "# GA의 평가 함수 정의\n",
        "def evaluate(individual):\n",
        "    n_estimators = int(individual[0])\n",
        "    max_depth = int(individual[1])\n",
        "    min_samples_split = max(2, int(individual[2]))  # 최소 2로 설정\n",
        "    min_samples_leaf = max(1, int(individual[3]))  # 최소 1로 설정\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=42\n",
        "    )\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    return mape(y_test, y_pred),\n",
        "\n",
        "# GA 설정\n",
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"n_estimators\", np.random.randint, 50, 200)\n",
        "toolbox.register(\"max_depth\", np.random.randint, 5, 50)\n",
        "toolbox.register(\"min_samples_split\", np.random.randint, 2, 20)  # 최소 2로 설정\n",
        "toolbox.register(\"min_samples_leaf\", np.random.randint, 1, 20)  # 최소 1로 설정\n",
        "\n",
        "toolbox.register(\n",
        "    \"individual\",\n",
        "    tools.initCycle,\n",
        "    creator.Individual,\n",
        "    (toolbox.n_estimators, toolbox.max_depth, toolbox.min_samples_split, toolbox.min_samples_leaf),\n",
        "    n=1\n",
        ")\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# 평가 함수 등록\n",
        "toolbox.register(\"evaluate\", evaluate)\n",
        "\n",
        "# 교차와 변이 등록\n",
        "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=[50, 5, 2, 1], up=[200, 50, 20, 20], eta=1.0, indpb=0.2)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "# 초기 개체군 생성 (기본 개체 + 최적 파라미터 개체)\n",
        "population_size = 20\n",
        "population = toolbox.population(n=population_size - 1)  # 기본 개체 생성\n",
        "population.append(creator.Individual([\n",
        "    best_params_from_random_search['n_estimators'],\n",
        "    best_params_from_random_search['max_depth'],\n",
        "    best_params_from_random_search['min_samples_split'],\n",
        "    best_params_from_random_search['min_samples_leaf']\n",
        "]))  # 최적 파라미터 개체 추가\n",
        "\n",
        "# GA 실행\n",
        "ngen = 40  # 세대 수\n",
        "cxpb = 0.5  # 교차 확률\n",
        "mutpb = 0.2  # 변이 확률\n",
        "\n",
        "result_population, logbook = algorithms.eaSimple(population, toolbox, cxpb, mutpb, ngen, verbose=True)\n",
        "\n",
        "# 최적의 하이퍼파라미터 찾기\n",
        "best_individual = tools.selBest(result_population, k=1)[0]\n",
        "print(\"Best individual is: \", best_individual)\n",
        "print(\"With MAPE: \", evaluate(best_individual)[0])\n",
        "\n",
        "# 최적의 파라미터로 모델 재학습\n",
        "optimal_rf = RandomForestRegressor(\n",
        "    n_estimators=int(best_individual[0]),\n",
        "    max_depth=int(best_individual[1]),\n",
        "    min_samples_split=max(2, int(best_individual[2])),  # 최소 2로 설정\n",
        "    min_samples_leaf=max(1, int(best_individual[3])),  # 최소 1로 설정\n",
        "    random_state=42\n",
        ")\n",
        "optimal_rf.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 모델 평가\n",
        "y_pred = optimal_rf.predict(X_test)\n",
        "final_mape = mape(y_test, y_pred)\n",
        "print(f\"Final MAPE with optimal parameters: {final_mape:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6nH2pUz6-pL"
      },
      "source": [
        "# GA - Jennie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaECyVyc687n"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from deap import base, creator, tools, algorithms\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume you have pre-processed data available in X and y\n",
        "X = combined_data.drop(columns=['Efficiency (Instantaneous)'])\n",
        "y = combined_data['Efficiency (Instantaneous)']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Best hyperparameters from RandomizedSearchCV\n",
        "best_params_from_random_search = {\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 15,\n",
        "    'min_samples_split': 5,\n",
        "    'min_samples_leaf': 2\n",
        "}\n",
        "\n",
        "# Define the objective function for GA to minimize (MAPE)\n",
        "def objective_function(individual):\n",
        "    n_estimators = int(individual[0])\n",
        "    max_depth = int(individual[1])\n",
        "    min_samples_split = max(2, int(individual[2]))\n",
        "    min_samples_leaf = max(1, int(individual[3]))\n",
        "\n",
        "    # Create the RandomForest model\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate MAPE\n",
        "    y_pred = rf.predict(X_test)\n",
        "    return mape(y_test, y_pred),\n",
        "\n",
        "# GA setup\n",
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))  # We are minimizing MAPE\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "\n",
        "# Register hyperparameters to be optimized\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_n_estimators\", random.randint, 50, 200)\n",
        "toolbox.register(\"attr_max_depth\", random.randint, 5, 30)\n",
        "toolbox.register(\"attr_min_samples_split\", random.randint, 2, 10)\n",
        "toolbox.register(\"attr_min_samples_leaf\", random.randint, 1, 5)\n",
        "\n",
        "# Define an individual and a population\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "                 (toolbox.attr_n_estimators, toolbox.attr_max_depth,\n",
        "                  toolbox.attr_min_samples_split, toolbox.attr_min_samples_leaf), n=1)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Include the best params from RandomizedSearchCV in the initial population\n",
        "population_size = 30\n",
        "population = toolbox.population(n=population_size - 1)\n",
        "population.append(creator.Individual([\n",
        "    best_params_from_random_search['n_estimators'],\n",
        "    best_params_from_random_search['max_depth'],\n",
        "    best_params_from_random_search['min_samples_split'],\n",
        "    best_params_from_random_search['min_samples_leaf']\n",
        "]))\n",
        "\n",
        "# Register Genetic Algorithm components\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)  # Crossover\n",
        "toolbox.register(\"mutate\", tools.mutUniformInt, low=[50, 5, 2, 1], up=[200, 30, 10, 5], indpb=0.2)  # Mutation\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)  # Selection\n",
        "toolbox.register(\"evaluate\", objective_function)  # Fitness evaluation\n",
        "\n",
        "# Early stopping criteria (optional)\n",
        "early_stop_threshold = 5\n",
        "no_improvement_counter = 0\n",
        "best_mape = float(\"inf\")\n",
        "\n",
        "# Define number of generations and probability parameters\n",
        "ngen = 15  # Fewer generations\n",
        "cxpb = 0.6  # Crossover probability\n",
        "mutpb = 0.2  # Mutation probability\n",
        "\n",
        "# Run the GA optimization\n",
        "for gen in range(ngen):\n",
        "    offspring = toolbox.select(population, len(population))\n",
        "    offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "    # Apply crossover and mutation\n",
        "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "        if random.random() < cxpb:\n",
        "            toolbox.mate(child1, child2)\n",
        "            del child1.fitness.values\n",
        "            del child2.fitness.values\n",
        "\n",
        "    for mutant in offspring:\n",
        "        if random.random() < mutpb:\n",
        "            toolbox.mutate(mutant)\n",
        "            del mutant.fitness.values\n",
        "\n",
        "    # Evaluate the offspring\n",
        "    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "    fitnesses = map(toolbox.evaluate, invalid_ind)\n",
        "    for ind, fit in zip(invalid_ind, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # Replace the old population with the new\n",
        "    population[:] = offspring\n",
        "\n",
        "    # Track the best solution and implement early stopping\n",
        "    best_individual = tools.selBest(population, k=1)[0]\n",
        "    current_mape = toolbox.evaluate(best_individual)[0]\n",
        "\n",
        "    if current_mape < best_mape:\n",
        "        best_mape = current_mape\n",
        "        no_improvement_counter = 0\n",
        "    else:\n",
        "        no_improvement_counter += 1\n",
        "\n",
        "    # Early stopping if no improvement for 5 generations\n",
        "    if no_improvement_counter >= early_stop_threshold:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "# Print the best individual and its MAPE\n",
        "best_individual = tools.selBest(population, k=1)[0]\n",
        "print(f'Best hyperparameters: n_estimators={best_individual[0]}, max_depth={best_individual[1]}, '\n",
        "      f'min_samples_split={best_individual[2]}, min_samples_leaf={best_individual[3]}')\n",
        "print(f'Best MAPE: {best_mape}')\n",
        "\n",
        "# Retrain the model with the best hyperparameters\n",
        "optimal_rf = RandomForestRegressor(\n",
        "    n_estimators=int(best_individual[0]),\n",
        "    max_depth=int(best_individual[1]),\n",
        "    min_samples_split=int(best_individual[2]),\n",
        "    min_samples_leaf=int(best_individual[3]),\n",
        "    random_state=42\n",
        ")\n",
        "optimal_rf.fit(X_train, y_train)\n",
        "\n",
        "# Final evaluation on test set\n",
        "y_pred = optimal_rf.predict(X_test)\n",
        "final_mape = mape(y_test, y_pred)\n",
        "print(f'Final MAPE with optimal parameters: {final_mape:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fBpE3ijHa7K6"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}